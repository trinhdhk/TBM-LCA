# Discussions

We implemented the Bayesian Latent Class Analysis for a population of `r nrow(data_19EI)` patients with suspected brain infection through which we estimated the performance of confirmatory assays. Ziehl-Neelsen Smear, despite its simplicity and availability, continues to prove its superiority over other two with `r sen('z_Smear')` of sensitivity, similar to prior studies [@nhu2013; @donovan2020]. GeneXpert, albeit developed and costly, performed poorly on CSF samples. All test performed significantly better in the HIV positive group, mainly due to higher bacillary burden. With senstivity and specificity both higher than 90%, either of the three tests can be used as a reliable gold standard for HIV patients. 

Compared with a recent study [@donovan2020] where the authors used one subset of our data with 305 patients, our estimated sensitivity were lower, especially for Xpert. In the paper, ZN Smear's sensitivity was estimated to be 71.3%, while MGIT and Xpert were 47.9% and 39.6% respectively. However, stratified by HIV status, the values were more comparable, as the sensitivity of Xpert were 22.9% (12.1%-39.0%) for HIV negative in the paper (compared to our estimates `r sen('z_Xpert', hiv=FALSE)`) and 76.9% (49.7%-91.8%) for HIV positive population (compared to our `r sen('z_Xpert', hiv=TRUE)`). This was because the sample in past study were TBM-suspected patients, so the number of HIV positive cases were expected to be much higher.

<!--One important output of the model is the actual TBM risk. Despite being design for research, due to the categorical nature, current TBM studies usually suffer difficulty in which group of patients should be considered as TBM. By providing the risk probability, our model can benefit both. A study of only definite TBM might favour severe patients while a lower group might have more false positive mixed with them. Choosing a cut-off usually comes with large trade-off, as a large group of patient would be excluded or included. By providing a probability, our model can benefit both clinicians and researchers with more clear insight, and more flexible choices of cut-off. In prognosis research, this probability, together with the ordinal quanitification of bacillary burden, can be incorporated as correction parameters for patients' responses to TBM treatment regimen, which is usually quite specific. 
=======
Through the implication of LCA, our model succesfully estimated the positive rates for corresponding confirmation procedures. We founds out that GeneXpert, in combination with Xpert Ultra, was not superior to other confirmatory tests, at least for TBM diagnosis, contrary to its popularity and benefit for pulmonary TB <citation>. Compared to estimations the most recent one [@donovan2020], where the authors used one subset of data with 305 patients, the estimated sensitivity were lower for all three assays, with ZN Smear stood at 67% (compared with 71.3%), MGIT culture 31% (47.9%), and Xpert was 13% (39.6%). We would argue that the latter were against standard uniform definition which is in favor of the confirmatory tests themselves, whereas ours were calculated against the actual TBM status, regardless of the test results. This might also be a suggestion of a small misdiagnosis of TBM when using the current approach, particularly for those with negative test results <may be we can calculate prob TBM in case all test negative to demonstrate this>. This underperfomance was again shown in the calibration plot against discharge diagnosis, with a small dip of observed risk at the around 50%. In fact, we all agree that none amongst the three confirmation indicators are reliable to be used alone as gold standard, and a true gold standard is urgently needed. They, however, all performs well in TBM negative class, with mostly no false positive. These estimations reflect well the biological mechanisms of the tests.
>>>>>>> Stashed changes-->

Our analysis was not the first to use LCA in TBM diagnosis. One study was done in Hanoi, Vietnam [@le2020] used data from the Vietnam National Lung Hospital. Compared with them, our target population was different, as the earlier estimated the prevalence of TBM amongst TB patients, where ours were TBM amongst suspected brain infection. In that study, they assumed all confirmatory tests were independent conditionally on TBM status -- namely two patents with TBM were assumingly had the same probability of test positive -- which was not necessarily true. Similar to another study [@moreira2008] dichotomised biomarkers were used as indicators. Our approach was methodically more flexible and allowed us to develop the most highlighted finding: a calibrated scoring system. We witnessed that some used predictors like TB-suggested symptoms and cranial nerve palsy had overestimated scores originally [@marais2010]. HIV - an essential factor, was not included. Laboratory parameters in general followed the same trend as prior knowledge. 

A novel quantification of our model is the ordinal manifestation of bacillary burden at baseline, roughly estimating TBM severity at baseline. Lymphocyte count in CSF intriguingly reduces the mycobacterial burden, which might also led to a less severe long-term prognosis, as pointed out elsewhere [@Thao2018]. Overally, the higher lymphocyte count in CSF, the more likely that the patients have a positive, but milder, TBM. 

All models were rigorously validated. Model 3, 4, and 5 showed relatively small difference in performance (supplementary document). Selected model shows great discrimination and calibration for all three confirmatory tests. Assuming that hospital diagnosis were not far from reality, our full model managed to surpass the most sensitive test Smear and our simplified model had comparable sensitivity using only clinical information. The calibration curves against three confirmatory tests showed some drop at the upper tails, particularly for MGIT culturing and Xpert. This drop was due to the scarcity of patients with extreme probability of positive tests. The calibration against the pseudo-gold hospital diagnosis showed some level of overestimation around $x=0.7$. This can be a suggestion of misdiagnosis, which motivated us to use the latent class analysis approach, but might also be an open question to further research, as there might be some hidden factors that we failed to capture.

This study have some limitations. One major challenges of our model is the amount of missing data, especially in HIV status. We had to make several assumptions, and despite some sensitivity analyses were made to fill the gap, this can be a huge factors that bias the results. Another disadvantage is the population from which our sample originated. Our study cohort was patients with suspected neurological infection, which implies some level of arbitrary, as different hospitals in different areas do not necessarily share the same judgement and experience. In that context, a further, multi-site, multi-national study is needed. And even if that risk is omittable, our sample can still not be representative for the whole population, and our TBM prevalence is potentialy overestimated. Neither in clinical practice nor research field would that be an issue, as usually they are the cohort of interest. However, when doing populational surveillance, this must be taken into account; as we would advise against extrapolation under such a circumstance. Rather, it is better to perform a Bayesian correction basing on the prevalence of neurological infection in the society. The third issue can come from the design of our models that is still heavily dependent on current knowledge. Although we tried to as many as possible factors to correct for hidden effect, accidentally fusing some amount of noise might impact on the overall estimations. Additionally, in some niche cases, the predicted credible intervals for TBM risk can be very wide. Although, as suggested by the results, the mean estimation alone can be used in practice with minimal trade-off, in research where the uncertainty should be incorporated and captured properly, this can render the model less useful. 
