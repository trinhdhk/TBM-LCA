# Discussions

We implemented the Bayesian Latent Class Analysis for a large population consisted of `r nrow(data_19EI)` patients with suspected neurological infection. Using different level of capacity, we managed to capture many non-linear correlations and hidden factors, while still maintain its full interpretability. Through this, we successfully estimated the positive rates for corresponding confirmation procedures. Ziehl-Neelsen Smear, despite its simplicity and availability, continues to prove its superiority over other two with `r sen('z_Smear')` of sensitivity[@nhu2013; @donovan2020]. GeneXpert, as being the most developed and costly, performed surprisingly poorly on CSF samples (`r sen('z_Xpert')`). MGIT culturing provides intermediate results (`r sen('z_Mgit')`) and is a good combination to Smear, due to its simplicity. However, one huge downside of culturing is its time gap, which may unnecessarily delay to diagnosis and hence treatment. Note that no test amongst three provides highly reliable sensitivity to be used alone; a novel true gold standard is as a results still urgently need. On the flip side, all three assays perform well in detecting non-TBM, with mostly no false positive; these reflect well the biological mechanisms of the tests.

Compared to the most recent study[@donovan2020], where the authors used one subset of our data with 305 patients, estimated sensitivities were lower for all three assays. In that analysis, ZN Smear was estimatedly 71.3% sensitive, while MGIT and Xpert was 47.9% and 39.6% respectively. Their confidence intervals and our credible intervals still overlapped, however. There are explanation for this discrepancy. First is the difference in denominator. In the earlier one, sensitivity were calculated against the current standard uniform case definition which is deliberately favoured the test results and left out a whole group with score lower than 6, whereas ours were calculated against the actual TBM status. This might also suggest a misdiagnosis of TBM when using the current approach, particularly for those with negative test results <may be we can calculate probs TBM in case all test negative to demonstrate this>; the underperformance was again visualised in the calibration plot against discharge diagnosis (figure \@ref(fig:theta-metrics)), with a small dip on the fitted line at the around $x = 0.5$.

<!--One important output of the model is the actual TBM risk. Despite being design for research, due to the categorical nature, current TBM studies usually suffer difficulty in which group of patients should be considered as TBM. By providing the risk probability, our model can benefit both. A study of only definite TBM might favour severe patients while a lower group might have more false positive mixed with them. Choosing a cut-off usually comes with large trade-off, as a large group of patient would be excluded or included. By providing a probability, our model can benefit both clinicians and researchers with more clear insight, and more flexible choices of cut-off. In prognosis research, this probability, together with the ordinal quanitification of bacillary burden, can be incorporated as correction parameters for patients' responses to TBM treatment regimen, which is usually quite specific. 
=======
Through the implication of LCA, our model succesfully estimated the positive rates for corresponding confirmation procedures. We founds out that GeneXpert, in combination with Xpert Ultra, was not superior to other confirmatory tests, at least for TBM diagnosis, contrary to its popularity and benefit for pulmonary TB <citation>. Compared to estimations the most recent one [@donovan2020], where the authors used one subset of data with 305 patients, the estimated sensitivity were lower for all three assays, with ZN Smear stood at 67% (compared with 71.3%), MGIT culture 31% (47.9%), and Xpert was 13% (39.6%). We would argue that the latter were against standard uniform definition which is in favor of the confirmatory tests themselves, whereas ours were calculated against the actual TBM status, regardless of the test results. This might also be a suggestion of a small misdiagnosis of TBM when using the current approach, particularly for those with negative test results <may be we can calculate prob TBM in case all test negative to demonstrate this>. This underperfomance was again shown in the calibration plot against discharge diagnosis, with a small dip of observed risk at the around 50%. In fact, we all agree that none amongst the three confirmation indicators are reliable to be used alone as gold standard, and a true gold standard is urgently needed. They, however, all performs well in TBM negative class, with mostly no false positive. These estimations reflect well the biological mechanisms of the tests.
>>>>>>> Stashed changes-->

Another finding of our study is the statistical-based estimation of individual TBM probability and contribution of current known diagnostic criteria. From our data, we witnessed that some used predictors like TB-suggested symptoms and cranial nerve palsy have minimal impact on the risk of TBM. This is contrary to previous consensual definition[@marais2010] where the former had a score of 2 and the latter of 1. On the other hand, HIV, albeit not mentioned in the original score, turns out to be very important, with an estimated coefficient of `r a_plot$data$m[1] |> round(2)`. The case of HIV has been well-known, to remedy this, usually, doctors just considered two sub-population differently, added a implicit bonus point for HIV positive patients.  TB-suggested symptoms, in spite of its name, are very non-specific and are prone to noise, while cranial nerve palsy might also appears in several neurological diseases. Laboratorial parameters in general follow the same trend as background knowledge.

A novel quantification of our model is the bacillary burden at baseline. Though it does not have a unit, the number can be a rough estimator for patient severity at baseline, given TBM. HIV was indeed a positive risk factor for severity, so were CSF white cell and lactate. Lymphocyte count in CSF, consistent to a recent study[@Thao2018], intriguingly reduces the bacilliary burden and improves long-term prognosis. In general, the higher Lymphocyte count in CSF, the more likely that the patients have a positive, but mild, TBM.

Basing on our posterior estimations, we proposed a mathematical formula to calculate TBM probability <cross-ref>. A Shiny app was built as a convenient tool for research purpose, which will cover the level of uncertainty. In clinical settings, we also provide a simplified scoring system, based on our simplification model. This system only needs demographic and clinical inputs, and are usually easier to diagnose, as early as admission time. Compared to the cartegorical risk level, an actual probability can provide clinicians with better insight, more flexibility to choosing who are to be included in a study, and vice versa. In prognosis research, the probability, in combination with the predicted bacillary burden, can be incorporated as correction parameters for patients' responses to TBM treatment regimen, which is usually quite specific.

```{r pseudo-fit}

```


```{r corrected-score, tab.cap="Corrected TBM definition, as compared the uniform case definition", tab.id="corrected-score"}



```

All models we considered were carefully and rigorously validated. None of them showed inconstistent results. Model 3, 4, and 5 showed relatively small difference in performance, which also suggested the robustness of our designed. The selected model 3 shows great discrimation and calibration for all three confirmatory tests. The calibration against the pseudo-gold standard diagnosis at discharge showed mild level of overestimation. However, as discussed above, this diagnosis was uncertain and error-proned, which motivated us to use the latent class analysis approach. Discrimation on this was still great with nearly 95% of AUC. On the flip side, this might leave an open question to further research, as there might very well be some hidden factors that we failed to capture. 

This study have some limitations. One major challenges of our model is the amount of missing data, especially in HIV status. We had to make several asssumptions, and despite some sensitivity analyses were made to fill the gap, this can be a huge factors that bias the results. Another disadvantage is the population from which our sample originated. Our study cohort was patients with *suspected* neurological infection, which implies some level of arbitrary, as different hospital in different areas do not necessarily share the same judgement and experience. In that context, a further, multi-site, multi-national study is in needed. And even if that is omittable, our sample can still not be respresentative for the whole population, and our probability is potentialy overestimated. Neither in clinical pratice nor research field would that be an issue, as usually they are the cohort of interest. However, when doing populational surveilance, this must be taken into account; as we would advise against extrapolation under such a circumstance. Rather, it is better to perform a Bayesian correction basing on the prevalence  of neurological infection in the society. The third issue can come from the design of our models that is still heavily dependent on current knowledge. Although we tried to as many as possible factors to correct for hidden effect, accidentally fusing some amount of noise might impact on the overall estimations. Additionally, in some niche cases, the predicted credible intervals for TBM risk can be very wide. Although, as suggested by the results, the mean estimation alone can be used in practice with minimal trade-offs, in research where the uncertainty should be incorporated and captured properly, this can render the model less useful. 
