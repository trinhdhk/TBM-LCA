# Discussions

We implemented the Bayesian Latent Class Analysis for a population of `r nrow(data_19EI)` patients with suspected brain infection through which we estimated the performance of confirmatory assays. Ziehl-Neelsen Smear again prove its superiority over the other two with `r sen('z_Smear')` of sensitivity, similar to prior studies [@nhu2013; @donovan2020]. GeneXpert performed poorly on CSF samples. All test performed significantly better in the HIV positive group, due to higher bacillary burden. With sensitivity and specificity both higher than 90%, either of the three tests can be used as a reliable standard for HIV patients. 

Compared with a recent study [@donovan2020] where the authors used one subset of our data with 305 patients, our estimated sensitivity were lower. In the paper, ZN Smear's sensitivity was estimated to be 71.3%, while MGIT and Xpert were 47.9% and 39.6% respectively. However, stratified by HIV status, our values were more comparable, as the sensitivity of Xpert were 22.9% (12.1%-39.0%) for HIV negative in the paper (compared to our estimates `r sen('z_Xpert', hiv=FALSE)`) and 76.9% (49.7%-91.8%) for HIV positive population (compared to our `r sen('z_Xpert', hiv=TRUE)`). This was because the past study targeted on TBM-suspected patients, HIV positive cases were more prevalent.

<!--One important output of the model is the actual TBM risk. Despite being design for research, due to the categorical nature, current TBM studies usually suffer difficulty in which group of patients should be considered as TBM. By providing the risk probability, our model can benefit both. A study of only definite TBM might favour severe patients while a lower group might have more false positive mixed with them. Choosing a cut-off usually comes with large trade-off, as a large group of patient would be excluded or included. By providing a probability, our model can benefit both clinicians and researchers with more clear insight, and more flexible choices of cut-off. In prognosis research, this probability, together with the ordinal quanitification of bacillary burden, can be incorporated as correction parameters for patients' responses to TBM treatment regimen, which is usually quite specific. 
=======
Through the implication of LCA, our model succesfully estimated the positive rates for corresponding confirmation procedures. We founds out that GeneXpert, in combination with Xpert Ultra, was not superior to other confirmatory tests, at least for TBM diagnosis, contrary to its popularity and benefit for pulmonary TB <citation>. Compared to estimations the most recent one [@donovan2020], where the authors used one subset of data with 305 patients, the estimated sensitivity were lower for all three assays, with ZN Smear stood at 67% (compared with 71.3%), MGIT culture 31% (47.9%), and Xpert was 13% (39.6%). We would argue that the latter were against standard uniform definition which is in favor of the confirmatory tests themselves, whereas ours were calculated against the actual TBM status, regardless of the test results. This might also be a suggestion of a small misdiagnosis of TBM when using the current approach, particularly for those with negative test results <may be we can calculate prob TBM in case all test negative to demonstrate this>. This underperfomance was again shown in the calibration plot against discharge diagnosis, with a small dip of observed risk at the around 50%. In fact, we all agree that none amongst the three confirmation indicators are reliable to be used alone as gold standard, and a true gold standard is urgently needed. They, however, all performs well in TBM negative class, with mostly no false positive. These estimations reflect well the biological mechanisms of the tests.
>>>>>>> Stashed changes-->

Our analysis was not the first to use LCA in TBM diagnosis. One study was done in Hanoi, Vietnam [@le2020] used data from the Vietnam National Lung Hospital. Compared with them, our target population was different, as the earlier assumably estimated TBM prevalence amongst general TB, where ours were amongst suspected brain infection. In that study, they assumed all confirmatory tests were independent -- namely two patents with TBM had the same probability of test positive -- which was not necessarily true. Different from past works [@moreira2008; @le2020], non-specific biomarkers were not used as manifest variables in our study but as predictors. Our method was more flexible and allowed us to develop the most highlighted finding: a calibrated scoring system. We witnessed that some used predictors like TB-suggested symptoms and cranial nerve palsy had overestimated scores originally [@marais2010], while HIV was omitted altogether. Laboratory parameters in general followed the same trend as prior knowledge. 

A novel quantification of our model is mycobacterial burden at baseline. Lymphocyte count in CSF intriguingly reduces the mycobacterial burden, which might also led to a less severe long-term prognosis, as pointed out elsewhere [@Thao2018]. Overally, the higher lymphocyte count in CSF, the more likely that the patients have a positive, but milder, TBM. 

All models were rigorously validated. Selected model shows great discrimination and calibration for all three confirmatory tests. Our full model managed to surpass the most sensitive test Smear and our simplified model had comparable sensitivity using only clinical information. The calibration curves against three confirmatory tests showed some drop at the upper tails, particularly for MGIT culturing and Xpert. This drop was due to the scarcity of patients with extreme probability of positive tests. The calibration against the pseudo-gold hospital diagnosis showed some level of overestimation around $x=0.7$. This suggests some misdiagnosis in practice, which motivates us to use the LCA, but might also due to some hidden factors we failed to capture. Further research is required.

This study have some limitations. One major challenges of our model is the amount of missing data, especially test results and HIV status. We had to make several assumptions, and despite the sensitivity analyses (supplementary document), this can underpower the result. Another disadvantage is the population from which our sample originated. Our study cohort was patients with suspected neurological infection, which implies some level of arbitrary. Even if that risk is omittable, our TBM prevalence is potentialy overestimated in the general population. Neither in clinical practice nor research field would that be an issue, as usually they are the cohort of interest. However, when doing population surveillance, this must be taken into account; as we would advise against extrapolation under such a circumstance. Rather, it is better to perform a correction basing on neurological infection prevalence. The third issue can come from our model design that is dependent on expertise and accidentally fusing some amount of noise could impact the overall estimates. 
