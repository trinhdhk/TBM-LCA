\clearpage
\pagebreak
\newpage

# (APPENDIX) Supplementary Documents {.unlisted .unnumbered}

# Statistical supplementary document {.unnumbered}

<!-- :::{custom-style="Title"} -->
<!-- Supplementary document -->
<!-- ::: -->
\pagenumbering{roman}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thepage}{S\roman{page}}

# Model Specification Details {#appendix-model-spec}

<!-- ## Overview -->
## Mathematical parametrisation {#mathematical-parametrisation}

Our model consists of two components:

1.  *Prevalence model*: A logistic regression model that quantifies the probability to have TBM (\(\theta\)) based on a set of diagnostic features ($X$):
  $$
    \mathrm{logit}(\theta) = X^T\alpha
  $$
  
  > where $X$ and $\alpha$ are vectors of covariate values and coefficients.

2.  *Indicator model*: three logistic regression models, one for each confirmatory test, that quantify the probability of having a positive test result depending on TBM status and additional variables as described in Supplementary Table \@ref(tab:model-archs). Similar to previous studies [@qu1996; @hadgu2002; @schumacher2016], we allow the test result to depend on the unobserved bacillary burden and systematic noise of the procedure. Both are in turn regressed on relevant bio-markers and an unobserved random effect. Presumably, TBM patients with lower bacillary burden are less likely to test positive. Our models 1,2 and 5 are similar in spirit to models M1, M3 and M4 in the previous paper [@schumacher2016]. In their models, they did not make the direct link between the covariates and bacillary burden, which is what we do in our models 3 and 4. These two models are in-between M3 and M4.

We give a detailed description of the structure of our indicator models. For $1 \leq i \leq 660$, let $C_i \in \{0,1\}$, and $\theta_i = P(C_i=1)$ be the *true TBM status* and the *probability of having TBM*; {$y^{ZN\mbox{-}Smear}_i$, $y^{MGIT}_i$, and $y^{Xpert}_i$} are the *observed test outcomes*. The joint distribution of the observed test results is:

$$
\begin{aligned}
P_i &= P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i) \\
&= \theta_i \times P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i|C_i=1)\\ 
 &\ \ \ \ + (1-\theta_i) \times P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i|C_i=0) 
\end{aligned} 
$$ {#eq:lca-fml}

Under the assumption that test results are mutually independent given TBM status, we obtain the likelihood function for Model 1 in Supplementary Table \@ref(tab:model-archs). The contribution to the likelihood for individual $i$ is:

$$
\begin{aligned}
P_i &= \theta_i \times P(y^{ZN\mbox{-}Smear}_i | C_i=1) \times P(y^{Mgit}_i | C_i=1) \times P(y^{Xpert}_i | C_i=1) \\
&\ \ \ \ + (1-\theta_i) \times  P(y^{ZN\mbox{-}Smear}_i | C_i=0) \times P(y^{Mgit}_i | C_i=0) \times P(y^{Xpert}_i | C_i=0)
\end{aligned}
$$ {#eq:classic-lca-fml}

This independence assumption is likely to be too strong, because all tests are based on the detection of *Mtb* in the CSF. Therefore, we introduce a latent variable, denoted as bacillary burden. For models 2 and 3, we assume that the test results are independent conditionally on TBM status and bacillary burden ($B$) [@schumacher2016]. Individuals without TBM have zero bacillary burden. The contribution to the likelihood, conditionally on bacillary burden $B_i$, becomes:

$$
\begin{aligned}
P_i| B_i &= P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i | B_i) \\
&= \theta_i \times P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i|C_i=1, B_i)\\ 
&\ \ \ \ + (1-\theta_i) \times P(y^{ZN\mbox{-}Smear}_i,y^{Mgit}_i,y^{Xpert}_i|C_i=0) \\
&= \theta_i \times P(y^{ZN\mbox{-}Smear}_i | C_i=1,B_i) * P(y^{Mgit}_i | C_i=1,B_i) \times P(y^{Xpert}_i | C_i=1, B_i) \\
&\ \ \ \ + (1-\theta_i) \times  P(y^{ZN\mbox{-}Smear}_i | C_i=0) * P(y^{Mgit}_i | C_i=0) \times P(y^{Xpert}_i | C_i=0)
\end{aligned} 
$$ {#eq:re-lca-fml}

Bacillary burden is defined as a unitless, standardised variable, which in turn depends on a set of features (denoted as modulating factors $V_i$) via a linear regression. We use a mixed-effects logistic regression to capture the association between $B$ and test results ($y^{(t)}$). In model 3 we assume:

$$
\begin{aligned}
\mathrm{logit}[P(y^{(t)}_i | C_i = 1, B_i)] &= z^{(t)}_0 + B_i \times \beta^{(t)} \\
&= z^{(t)}_0 + (V_i^T \gamma + r_i) \beta^{(t)}
\end{aligned} 
$$ {#eq:bd}

where $(t) \in$ \{ZN-Smear, Mgit, Xpert\}, $z^{(t)}_0$ is the intercept, and $\beta^{(t)}_{B}$ is the coefficient of bacillary burden on the result of test $(t)$, $\gamma$ is the vector of coefficients for $V_i$, and the individual random effect $r_i$ quantifies the fluctuation of bacillary burden not explained by $V_i$. $V_i$ and $r_i$ are assumed to be independent of each other. Model 2 is a special case of model 3 in which the $\beta^{(t)}$ are equal over $(t)$, which is the same as Model M3 in the previous study [@schumacher2016]. Model 3 assumes that for each pair of modulating factors, although their impact on test results (as expressed by $\beta^{(t)}$) could differ by test, their relative contribution (as measured by the pairwise ratios of their corresponding parameters) is constrained to be identical:

$$
\frac{\gamma^{[k]} \beta^{(t)}}{\gamma^{[l]} \beta^{(t)}} = \frac{\gamma^{[k]}}{\gamma^{[l]}}
$${#eq:constr}

for each two elements $\gamma^{[k]}$ and $\gamma^{[l]}$ of vector $\gamma$.

In model 4, we additionally capture the sample-wise fluctuation that is not explained by the bacillary burden (Formula @eq:model-4). This fluctuation is modelled by one extra random effect ($u_i$), with coefficient $\upsilon^{(t)}$. Contrary to $r_i$ and  $\beta^{(t)}$, it is not related to the modulating factors $V_i$. unique for each individual that is not related to the modulating factors $V$.

$$
\begin{aligned}
\mathrm{logit}[P(y^{(t)}_i | C_i = 1, B_i)]  &= z^{(t)}_0 + B_i \times \beta^{(t)} + \upsilon^{(t)} u_i \\
&= z^{(t)}_0 + (V_i^T \gamma + r_i) \beta^{(t)} + \upsilon^{(t)} u_i
\end{aligned} 
$$ {#eq:model-4}

In model 5 (@eq:model-5), we explored all potential impacts of $V_i$ to test results by linking them directly to $P(y^{(t)}_i) | C_i = 1, V_i)$. This is the most flexible design and most similar to model M4 [@schumacher2016]. It doesn't constrain the relative contribution of each pair of modulating factors to be the same for all tests as in @eq:constr. 

$$
\begin{aligned}
\mathrm{logit}[P(y^{(t)}_i | C_i = 1, V_i)] &= z^{(t)}_0 + V_i^T \gamma^{(t)} + \upsilon^{(t)} r_i 
\end{aligned} 
$$ {#eq:model-5}

```{r model-archs, tab.cap="Different model designs", tab.id="model-archs"}
model_archs <-
  data.frame(
    Model=1:5,
    Def=c(
      'No bacillary burden; everyone in the same TBM class has equal risk to test positive',
      'Test results depend on individual bacillary burden; impact of bacillary burden on test result is the same across all tests',
      'Impact of bacillary burden differs between tests',
      'Added technical fluctuation as a second random effect; fixed effects only contribute to bacillary burden',
      'modulating factors also contribute to technical fluctuation (i.e. direct associtations between V and P(Y), not mediated by mycobacillary burden)'
    )
  ) 

model_archs |> 
  flextable::flextable() |>
  flextable::set_header_labels(
    Model='Model', 
    Def='Base description (Only additional effects compared to the preceded number are mentioned)'
  ) |>
  flextable::width(j=1, width=.5)|>
  flextable::align(j=1, align='center') |>
  flextable::width(j=2, width=5) |>
  flextable::theme_vanilla()
```

Posterior distributions were estimated using 4 chains, each with 2000 warm-up and 8000 effective iterations with a thinning of 2 (i.e. keeping every second iteration).

## Simplified model

The simplified prevalence model only includes diagnostic features for which a lumbar puncture is not needed. We didn't fit another latent class model, because it is likely to give worse TBM allocation than the LCM based on the full set of variables. Instead, we generated the response variable $\hat{C}_i \in \{0, 1\}$ from the posterior distribution of the individual probability $\theta_i$ to have TBM as fitted from the best performing model in Supplementary Table \@ref(tab:model-archs) (see Supplementary Table \@ref(tab:elpd-tbl)). The generated values were regressed on the restricted set of diagnostic features $K$. Hence:

$$
\begin{aligned}
\hat{C}_i &\sim Bernoulli(\hat \theta_i)\\
\mathrm{logit}[P(\hat{C}_i=1)] &\sim K_i^T\kappa
\end{aligned}
$$

 > where $\kappa$ is the vector of coefficients for the covariates $K$

To do this in `Stan`, we first generated 400 sets of individual TBM statuses $\hat{C}_1, \ldots \hat{C}_660$ from the selected LCM. Then we related each set to the diagnostic features (with missing values completed in the best performing LCM model in Supplementary Table \@ref(tab:model-archs). Per set, we ran one MCMC chain with 2000 warm-up and 4000 effective iterations. Finally, we combined all these chains for posterior summaries.

## Preprocessing and imputation {#appendix-data}

### Data preprocessing

The choice of diagnostic features $X$ and modulating factors $V$ was inspired by the uniform case definition [@marais2010]. Prior knowledges of different predictors were shown in Supplementary Table \@ref(tab:predictor-tab). Days from symptom onset to admission and CSF biomarkers were transformed to a logarithmic scale as they were right-skewed. We followed Gelman's scale-matching method [@gelman2008], n which continuous predictors were centred by their empirical means and subsequently scaled by 2 times their standard deviations (sd) while binary ones were kept as is. Since CSF eosinophil count was heavily zero-inflated, we created an extra indicator for count > 0 and rescaled the positive values by their sd. Glasgow Coma Score (GCS) and its components (Voice - GCSV, Eyes - GCSE, and Muscle - GCSM) were translated to *Reversed GCS* (RGCS = RGCSV + RGCSE + RGCSM) so that a $GCS = 15$ would be equivalent to $RGCS = 0$, while $GCS = 3$ would be translated to $RGCS = 12$ (Supplementary Table \@ref(tab:gcs-tab)). This transformation facilitated the choice of prior in the imputation process as all of them shared the minimum of 0 for the healthy condition (instead of starting at arbitrary numbers) and also improved numerical stability during sampling. We scaled down the auxiliary variables RGCSE, RGCSM, and RGCSV to the range $[0, 1]$. Hence $RGCS = 3 RGCSE + 4 RGCSV + 5 RGCSM$. Binary variables were dummy-coded into 0 for "Negative" / "No" and 1 for "Positive" / "Yes".

\newpage

```{r gcs-tab, out.width='100%', tab.id='gcs-tab', tab.cap='Conversion table from classical Glasgow coma scales to Reversed Glasgow coma scales'}

tibble::tribble(
  ~ Feature,                     ~ Response, ~ GCS, ~ RGCS,
  'Eye response',        'Open spontaneously', 4, 0,
  'Eye response',     'Open to voice command', 3, 1,
  'Eye response',              'Open to pain', 2, 2,
  'Eye response',               'No eye open', 1, 3,
  'Verbal response',             'Orientated', 5, 0,
  'Verbal response',               'Confused', 4, 1,
  'Verbal response',     'Inappropriate words', 3, 2,
  'Verbal response','Incomprehensible sounds', 2, 3,
  'Verbal response',     'No verbal response', 1, 4,
  'Motor response',            'Obey command', 6, 0,
  'Motor response',         'Localising pain', 5, 1,
  'Motor response',    'Withdrawal from pain', 4, 2,
  'Motor response',                 'Flexing', 3, 3,
  'Motor response',               'Extending', 2, 4,
  'Motor response',       'No motor response', 1, 5
) |>
  flextable::flextable() |>
  flextable::merge_v(j = 1) |>  
  flextable::width(j=1, width=2) |>
  flextable::width(j=2, width=2) |> 
  flextable::theme_vanilla()
```

### Imputation strategy {#stat-impute}

Our imputation models are summarised in Supplementary Figure \@ref(fig:impute-model), together with the variables that were used as regressors. For each incomplete variable, the missing values were sampled from a distribution fitted to corresponding observed values, which formed part of the MCMC sampling procedure. Potentially correlated variables were grouped together and sampled from a multivariate distribution. Numerical features were imputed using Multivariate linear regression:

$$
X_{cont} \sim \mathcal{N}(L^T\psi, \Sigma)
$$ {#eq:cont-sim}

where $X_{cont}$ is the vector of continuous variables with missing values, and $L^T$ is the vector of regressors used in the respective imputation model, with coefficients $\psi$. $\mathcal{N}$ is the Normal distribution in univariate case and Multivariate Normal distribution otherwise with $\Sigma$ as the variance-covariance matrix. Composite variables (*TB-suggestive symptoms*, *local neurological deficit*, and *RGCS*) were imputed via their corresponding compartments. The method used was, however, different between binary (*TB-suggestive symptoms* and *local neurological deficit*) and ordinal variables (*RGCS*), in lights of the technical difficulty: 

  a. For *TB-suggestive symptoms* and *local neurological deficit*, we built a small LCM in which the composite variable was the latent class whose manifest variables were the corresponding components. Relevant regressors were added to estimate the class probability, as in the prevalence model. Strictly say, the classic LCM does not reflect the correct relationship, as the composite variables, by their definition, are *true* when and only when at least one component is *true*, i.e.
    
  $$
    X = \mathrm{I}(\sum_{j=1}^J{X_j} > 0)
  $$ 
    
where $X$ in the composite variable whose $J$ components are $X_j$, and I is the indicator function. Hence, we added two extra constraints to the manifest variables: (i) the false positive rates of all $X_j$ must be 0 (i.e. if $P(X_j=1 | X=0) = 0$), and (ii), without losing generality, given $X = 1$, the true positive rate of the last manifest variable is the probability of all other manifest variables negative, i.e. $P(X_J = 1 | X = 1) = \prod_{j=1}^{J-1}P(X_j=0|X=1)$ for j = 1..j-1. This is obvious because we know that at least one manifest variable must be 1 given $X = 1$, hence we have
  
$$
\begin{aligned}
1 - P(X_J = 1|X=1) &= P(X_J = 0|X=1)\\
 &= 1 - \prod_{j=1}^{J-1}P(X_j=0|X=1)
\end{aligned}
$$

Strictly judged, this is an imperfect method, because we assumed that as long as at least one in the three compartments is observed negative, the others are independently distributed. However, as we are only interested in the composite variables, is does not matter whatever value the missing compartments receive, as the composite is definitely positive.

  b. For *RGCS*, we imputed it components via a Multivariate Normal distribution as if they are continuous variables in the sampling step, similar to [@eq:cont-sim]. However, when estimating the model performance and doing inference, the values were rounded to nearest integers to reflect the nature of the features.

```{dot impute-model, echo=FALSE, out.width="100%", fig.align="center", fig.cap="Imputation strategy for variables with missing values. Variables contributing to the LCM are in white boxes with solid borders. Variables in ovals with dashed borders were used in the imputation model. They make up the composite variables (mentioned in the top of the grey boxes that were used in the LCM).  Variables in a grey box were imputed together via a multivariate regression. Arrows demonstrate covariate $\\rightarrow$ response relations. \"Mycobacterial test done\" is defined as \"ZN-Smear, MGIT and Xpert were done for the patient\""}

digraph imp_model{
  rankdir = TB;
  resolution=500;
  compound=true;
  # concentrate=true;
  splines=ortho;
  graph[fontname="CMU-Serif"];
  edge[fontname = "CMU-Serif"];
  node[fontname = "CMU-Serif"];
  
  subgraph cluster_csf{
    rank=same;
    label = "Lumbar puncture";
    bgcolor = lightgrey;
    node[style=filled fillcolor=white];
    subgraph glu{
      peripheries=0;
      CsfGlu;
      CsfLymp;
    }
    BldGlu;
    CsfNeu;
    subgraph bio{
      peripheries=0;
      CsfPro;
      CsfLac;
    }
  }
  
  subgraph cluster_clinsymp{
    rank=same;
    label = "TB-suggestive symptoms";
    bgcolor = lightgrey;
    node[style="filled,dashed" fillcolor=white];
    Cough[label="Coughing"];
    subgraph lowerz{
      peripheries=0;
      Nsweats[label="Night Sweats"];
      WLoss[label="Weight Loss"];
    }
  }
  
  subgraph cluster_motor{
    rank=same;
    label = "Focal neurological deficit";
    bgcolor = lightgrey;
    node[style="filled,dashed" fillcolor=white];
    Hemi[label="Hemiplegia"];
    subgraph lowerz{
      peripheries=0;
      Para[label="Paraplegia"];
      Tetra[label="Tetraplegia"];
    }
  }
  
  subgraph cluster_gcs{
    label = "Reversed GCS";
    bgcolor = lightgrey;
    node[style="filled,dashed" fillcolor=white];
    RGCSV;
    subgraph lowerz{
      peripheries=0;
      RGCSM; RGCSE;
    }
  }
  # 
  # subgraph cluster_bld{
  #   # peripheries=0;
  #   bgcolor = lightgrey;
  #   rank=same;
  #   node[style="filled,dashed" fillcolor=white];
  # }
 
  obs[label="Mycobacterial test done", shape="box"];
  HIV[label="HIV Status", shape="box"];
  # Age[label="Age", shape="box"];
  TBDays[label="Symptom duration", shape="box"]; 
  BldGlu[label="Blood glucose", shape="box"];
  CsfGlu[label="CSF glucose", shape="box"]; 
  CsfLymp[label="CSF lymphocyte count", shape="box"];
  CsfNeu[label="CSF neutrophil count", shape="box"];
  CsfPro[label="CSF protein", shape="box"];
  CsfLac[label="CSF lactate", shape="box"];
  # GCS[label="Glasgow Coma Score", shape="box"];
  
  obs -> HIV
  obs -> TBDays;
  obs -> Hemi[lhead=cluster_motor];
  obs -> Cough[lhead=cluster_clinsymp];
  obs -> RGCSV[lhead=cluster_gcs];
  obs -> CsfGlu[lhead=cluster_csf];
  HIV -> TBDays;
  HIV -> Hemi[lhead=cluster_motor];
  HIV:w -> Cough[lhead=cluster_clinsymp];
  HIV -> RGCSV[lhead=cluster_gcs];
  HIV -> CsfLymp[lhead=cluster_csf];
  TBDays -> RGCSV:w[lhead=cluster_gcs];
  TBDays -> CsfLymp:e[lhead=cluster_csf];
  
  TBDays -> Cough[lhead=cluster_clinsymp];
  TBDays -> Hemi[lhead=cluster_motor];
  
  Cough -> Nsweats[dir=none];
  Nsweats -> WLoss[dir=none];
  WLoss -> Cough[dir=none];
  Hemi -> Para[dir=none];
  Para -> Tetra[dir=none]; 
  Tetra -> Hemi[dir=none];
  
  CsfGlu -> BldGlu[dir=none];
  CsfGlu -> CsfLymp[dir=none];
  CsfLymp -> CsfNeu[dir=none];
  CsfNeu ->CsfPro[dir=none];
  CsfPro -> CsfLac[dir=none];
  BldGlu -> CsfLac[dir=none];

  RGCSV -> RGCSE[dir=none];
  RGCSE -> RGCSM[dir=none];
  RGCSM -> RGCSV[dir=none];
}  
```

There are, though, some exceptions: headache, neck stiffness, and psychosis (Supplementary Table \@ref(tab:missing-handling)). All of them only appear in the simplified prevalence model and were imputed using predictive mean matching method [@mice]. The imputation is done 400 times separately for 400 training datasets.Their donors are other complete variables imputed within the full model. 

## Choice of priors {#appendix-stat-prior-choices}

### Main model

For the prevalence model, we chose a Student's *t* distribution with 6 degrees of freedom ($t_6$) for the intercept, in which we fixed the mean to 0 and scale to 3 because its absolute value is unlikely to be higher than 6 (which is equivalent to an probability range of (0.002 - 0.998)) [@boonstra2019]. For the regression coefficients, we used *t* distributions with mean set to 0 and as scales we chose a prior $Half\mbox{-}Normal(0, 1.2)$. This acts as a shrinkage term, reflecting our belief that the absolute values of the coefficients are smaller than 4.8 (which is equivalent to an odds ratio range of 0.008 - 121) [@prior-choice; @vanerp2019], but we left some room for our belief to be wrong. For high-impact diagnostic features (marked as "strong" in Supplementary Table \@ref(tab:predictor-tab)), priors were truncated at 0, so that only values in the expected direction were accepted in the MCMC chains. Hence we chose:

$$
\begin{aligned}
\sigma &\sim Normal(0, 1) \\
\alpha_0 &\sim t_6(0, 3)\\
\alpha_{binary} &\sim t_6(0, \sigma)\\
\alpha_{continuous} &\sim t_6(0, \frac{\sigma}{2sd})
\end{aligned}
$$

where $\sigma$ is the adaptive penalty term; $\alpha_0$, $\alpha_{binary}$, and $\alpha_{continuous}$ are the intercept, coefficients for the binary covariates, and coefficients for the continuous covariates included in the prevalence model; $sd$ is the empirical standard deviation of the continuous variables after the imputation and differs by variables.

For the indicator model, our choice of priors was based on information collected from several previous studies [@nhu2013; @thwaites2004; @heemskerk2018].  A visual justification is given in Supplementary Figure \@ref(fig:mv-priors), where our chosen prior distributions are compared with  results  from  previous studies. As suggested by the TBM literature, we used highly informative priors for false positive rate ($FPR = 1-\mbox{Specificity}$) and weakly informative priors for true positive rate ($TPR = \mbox{Sensitivity}$), given the discrepancies in TPR between the studies (@eq:priors_response).

$$
\begin{aligned}
  1-Spc_{Xpert} &\sim Logistic(logit(0.005), 0.7) \\
  1-Spc_{MGIT}  &\sim Logistic(logit(0.001), 1) \\
  1-Spc_{ZN\mbox{-}Smear} &\sim Logistic(logit(0.001), 1) \\ 
  Sen_{Xpert},\ Sen_{MGIT},\ Sen_{ZN\mbox{-}Smear} &\sim Logistic(0, .5)
\end{aligned}
$$ {#eq:priors_response}


For coefficients $\gamma$ that form part of the modulating factor model,  we applied the same rules as in the prevalence model. Both latent random effects $r$ and $u$ were assumed to follow a Standard Normal distribution. To avoid sign switching, we constrained $\beta^{(t)}$ and $\upsilon^{(t)}$ to be non-negative. As coefficients $\gamma$ and $\beta^{(t)}$ were multiplied together, to provide a fair penalisation, we divided the penalty term $\sigma'$ for $\gamma$ by the average of $\beta^{(t)}$ over the three tests *t = {ZN-Smear, Mgit, Xpert}*.

Hence we chose

$$
\begin{aligned}
r &\sim Normal(0, 1) \\
u &\sim Normal(0, 1) \\
\sigma' &\sim Normal(0, 1) \\
\beta^{(t)} &\sim Normal(0, \sigma') \\
\upsilon^{(t)} &\sim Normal(0, \sigma') \\
\gamma_{HIV} &\sim t_6(0, \sigma'/\overline{\beta^{(t)}}) \\
\gamma_{continuous} &\sim t_6(0, \frac{\sigma'}{2sd \times \overline{\beta^{(t)}}})
\end{aligned}
$$

where $\gamma_{HIV}$ is the coefficient for HIV status; $\gamma_{continuous}$ is the vector of coefficients for other CSF biomarkers; $\sigma'$ is the penalty term for the bacillary burden model; $\overline{\beta^{(t)}}$ is the average of $\beta^{(t)}$ over (t); $sd$ is the empirical standard deviation of each continuous variable after the imputation and differs per variable.

### Imputation model

For the imputation models, $Normal(0, 2.5)$ was chosen as prior distributions for intercepts and coefficients [@prior-choice], except for RGCS (E, V, and M) as their values are constrained on both sides (Section \@ref(data-preprocessing)). The modes of RGCSV, RGCSM, and RGCSE were sampled from an $Exponential(\beta=20)$ truncated within [0, 1], and $Normal(0, 0.25)$ was chosen as the prior distribution for their sd.

In cases where imputed variables were assumed multivariably correlated, Cholesky decomposition of the variance-covariance matrix was sampled from a *Cholesky Lewandowski-Kurowicka-Joe (LKJ) Correlation* prior with $\eta = 4$ [@stan-doc].

# Model diagnosis and comparison

For each parameter we evaluated Brooks-Gelman-Rubin $\hat{R}$ statistic, effective sample size per individual and Monte Carlo standard errors.  

Model performance was estimated and compared using expected log pointwise density (elpd) [@vehtari2016]. This measure quantifies how likely a new observation (\(y_{new}\)) can be observed given the data $y$, model $\mathcal{M}$, and posterior distributions of the parameters $\theta$. In our case, ($y_{new}$) are the held-out observations. A larger elpd means that the predicted posterior distribution concentrates around the observed response value.

All elpd, ROC curves, AUCs, and calibration metrics are calculated using 5 repetitions of 20-fold cross validation. For each repetition, we pooled all held-out folds together to reconstruct a complete dataset, with their predictions coming from the model trained on the complement of the held-out folds. The averaged performance measures were acquired by pooling all held-out folds over the 5 repetitions together.

The cross validation procedure for the simplified model was done in the same manner, but adjusted for the fact that the outcome $\hat{C}_i$ is generated from the posterior $\theta_i$; we simulated a different response vector \(\hat{C}_i^k\) (of size 400) for every fold $k$ in each repetition $i$ in which $1 \leq k, i \leq 20$. The final AUC, ROC, and calibration were averaged over these $20\times20=400$ experiments.

As AUCs and calibrations of the selected model are reported in the main text, here we only report the results of the elpd measure for model comparison.

## Estimated log-pointwise density of each considered model

```{r elpd-tbl, tab.id='elpd-tbl', tab.cap = 'Expected elpd of considered models and their standard errors. Model 3 and 4 have comparable elpd. Model 3 is selected because it is simpler.'}
elpd =  readRDS(file.path(data_dir, '..', 'export/metrics/elpd.RDS'))
elpd_tbl =
  data.frame(
    Model = names(elpd) |> substr(2,2) |> as.integer(),
    elpd = sapply(elpd, \(x) x$estimates[,'Estimate'] |> formatC(format = 'f', digits=2)),
    SE = sapply(elpd, \(x) x$estimates[,'SE']  |> formatC(format = 'f', digits=2))
  ) |> arrange(Model)

elpd_tbl |>
  flextable::flextable()|>
  flextable::width(width=1.5) |>
  flextable::set_header_labels(SE = 'Standard error') |>
  flextable::highlight(i = 3, j = 2) |>
  flextable::theme_vanilla()
```

## Sampling diagnosis for the selected model

The histogram of the Brooks-Gelman-Rubin $\hat{R}$ statistic, effective sample size, and Monte Carlo standard errors across all parameters are shown in Supplementary Figure \@ref(fig:nut-dx) [@stan-doc]. A good mixing between chains should have $\hat{R}$ near 1, and a Monte Carlo standard error near 0. As MCMC sampling process contains some auto-correlation, effective sample size measures how many Bayesian samples do we have if the sampling process is independent. As a rule of thumbs, a value of at least 400 could be sufficient to approximate the true distribution [@stan-doc]. Overall, no signs of poor chain mixing and unreliable sampling of the posteriors were suggested by the diagnostics plots, with most $\hat{R}$ highly concentrated around $1 \pm .005$ and effective sample size $\geq$ 50% of the data sample size.

```{r load_data, warning=FALSE, include=FALSE}
stanDx <- c(readRDS(file.path(data_dir, '../export/metrics/m3_diag.RDS')), readRDS(file.path(data_dir, '../export/metrics/s_diag.RDS')))
```

```{r nut-dx, fig.align='center', fig.cap="Sampling diagnostic statistics for the selected full model (LHS) and the simplified prevalence model (RHS)", fig.id="nut-dx", warning=FALSE, out.width='80%', fig.width=8, fig.asp=1.5, message=FALSE}

wrap_plots(stanDx, ncol=2, byrow=F)
```

## Perfomance validation of the selected model on observed manifest variables (i.e. mycobacterial tests)

The performance of the prevalence model validated on the final hospital diagnosis was performed as a pseudo-validation under the assumption that all the doctors' diagnosis is a perfect gold standard, i.e. 100% sensitive and 100% specific. This may not be true and is the motivation to implement LCM. This supplementary section demonstrates the actual performance of the selected model against the observed mycobacterial tests. 

```{r model-metrics-2, fig.align="center", fig.cap='Performance of the selected indicator model (A and B) and the prevalence model (C and D). A: ROC and AUC against confirmatory test results (indicator model); B: calibration against confirmatory test results. For calibration plots, “observed” probabilities are defined as values smoothed by a loess fit against observed events. For ROC plots, AUC values are presented as “average (min - max over 5 repetitions of cross- validation)”. The grey lines are fitted curves from each 20-fold cross validation and coloured lines represent their average' , fig.dim=c(16, 12), message=FALSE, warning=FALSE, out.width='100%', fig.id="model-metrics-2"}

roc_y <- do.call(gridExtra::arrangeGrob, m3$ROC$Y)
m3$calib$Y[[3]] = m3$calib$Y[[3]] + theme(axis.title.y = element_blank())
(wrap_elements(roc_y) / wrap_elements(m3$calib$Y)) /
  plot_layout(heights=c(2,2)) +
  plot_annotation(tag_levels = 'A')

```

## Scaled Brier scores of the prevalence models against the final hospital diagnosis

```{r}
#| label: scaled_brier_scor
#| message: false
#| warning: false

scaled_b <- {
  theta <- m3$calib$C$plot_env$pred
  Y <- m3$calib$C$plot_env$obs
  lambda <- attr(metrics_s$calib, 'grobs')$panel$plot_env$pred
  prev <- sum(Y)/length(Y)
  
  brier_full <- mean((theta-Y)^2)
  brier_sim  <- mean((lambda-Y)^2)
  list(
    scaled_B_full = (prev - brier_full)/prev,
    scaled_B_sim = (prev - brier_sim)/prev
  )
}
 

```

This section compares the scaled Brier scores (@eq:brier) or explained residual variation of the full and simplified prevalence models against the pseudo-gold standard final hospital diagnosis. The scaled Brier score for the full model is `r formatC(scaled_b$scaled_B_full, format='f', digits=2)` and for the simplified prevalence model `r formatC(scaled_b$scaled_B_sim, format='f', digits=2)`.

$$
\begin{aligned}
B &= \frac{1}{N}\sum_{i=1}^N{(\hat{y_i}-y_i)^2}\\
R^2 &= \frac{p-B}{p}
\end{aligned}
$$ {#eq:brier}

> where $B$ is the Brier score, $y_i = {0,1}$ is the observed binary outcome, $0<\hat{y}_i<1$ is the predicted risk, and $p$ is the apparent prevalence of the outcome. The scaled Brier score $R^2$ depicts how much discriminative values gained against the Null model, in which everyone is predicted to have the same risk = prevalence. The worst value is 0 (the model of interest is identical to the Null model) and the best value is 1 (the model of interest has perfect prediction).


# Assumption checks and sensitivity analysis {#appendix-sen-analysis}

## Methods

### Latent class model 

*Independence of test results.* Apart from model performance metrics, we also checked our conditional independence assumptions, both without (model 1) and with correction for latent bacillary burden (model 2-5). To do this, we plotted the residual pairwise correlation between confirmatory tests, effectively comparing predicted pairwise correlation and observed pairwise correlation  [@qu1996].

*Near-perfect specificity of GeneXpert.* GeneXpert might not have perfect specificity [@nhu2013]. Then some patients with missing Xpert that did not have TBM would have a positive test result. To check this, we did a sensitivity analysis in which we added the presence of an Xpert test as fourth manifest variable to the indicator model. We assumed that observation of Xpert only depends on TBM status of the patient. It is denoted as *obs*. If $obs = 0$, which means Xpert is missing, we marginalised out

$$
\begin{aligned}
&P(y^{Xpert}_i = x,y^{ZN\mbox{-}Smear}_i = s,y^{Mgit}_i = m | C_i=c, B_i, obs_i=0) \\
&= \sum_{x=0}^1{P(y^{Xpert}_i = x | y^{ZN\mbox{-}Smear}_i = s,y^{Mgit}_i = m,C_i=c,B_i,obs_i=0)P(y^{ZN\mbox{-}Smear}_i = s,y^{Mgit}_i = m|C_i=c,B_i, obs_i=0)} \\
&= P(y^{ZN\mbox{-}Smear}_i = s,y^{Mgit}_i = m|C_i=c,B_i,obs_i=0)
\end{aligned}
$$

(as Xpert is independent to ZN-Smear and MGIT given $C$, $B$ and $obs$, where $s, m, c, o = {0,1}$ are observed values of $y^{ZN\mbox{-}Smear}$, $y^{Mgit}$, $C$, and *obs* for individual $i$, respectively). The likelihood function (@eq:lca-fml) becomes


$$
\begin{aligned}
P_i &= P(y^{ZN\mbox{-}Smear}_i=s,y^{Mgit}_i=m,y^{Xpert}_i=x, {obs}_i=o)\\
&= \theta_i \times I(obs_i=1) \times P(obs_i=1|C_i=1) \times P(y^{ZN\mbox{-}Smear}_i=s,y^{Mgit}_i=m,y^{Xpert}_i=x|C_i=1, obs_i=1)\\ 
&\ \ \ \ + \theta_i \times I(obs_i=0) \times (1-P(obs_i=1|C_i=1)) \times P(y^{ZN\mbox{-}Smear}_i=s,y^{Mgit}_i=m|C_i=1, obs_i=0)\\ 
&\ \ \ \ + (1-\theta_i) \times I(obs_i=1) \times P(obs_i=1|C_i=0) \times P(y^{ZN\mbox{-}Smear}_i=s,y^{Mgit}_i=m,y^{Xpert}_i=x|C_i=0, obs_i=1) \\
&\ \ \ \ + (1-\theta_i) \times I(obs_i=0) \times (1-P(obs_i=1|C_i=0)) \times P(y^{ZN\mbox{-}Smear}_i=s,y^{Mgit}_i=m|C_i=0, obs_i=0) 
\end{aligned} 
$$ {#eq:lca-missingXpert-fml}

given that _obs_ is locally independent to the three confirmatory tests. We used a weakly informative prior distribution for the non-TBM group and an informative prior distribution for the TBM group (Supplementary Figure \@ref(fig:prior-obs)) as we believe most patients in the TBM group have been tested with the confirmatory tests. However, we widened the scale enough so that an extreme value of 50% TBM patients who left untested was allowed.

```{r prior-obs, fig.align='center', out.width='100%', warning=FALSE, message=FALSE, fig.cap = "Prior distribution for Xpert response rate, in non-TBM patients (top) and TBM patients (bottom). Point, thick lines, and thin lines are the median, 50\\% and 95\\% inter-percentile ranges. Logit scale is used along the x-axis"}

obs_rng = rbind(
  data.frame(
    Test = 'Xpert observed / non-TBM',
    Test_id = 1,
    logit = rlogis(500000, 0,.5),
    linear = rlogis(500000, 0,.5) |> plogis()
  ),
  data.frame(
    Test = 'Xpert observed / TBM',
    Test_id = 2,
    logit = rlogis(500000, qlogis(.9),1),
    linear = rlogis(500000, qlogis(.9),1) |> plogis()
  )
) 


obs_logit_plt <- ggplot() + 
  ggdist::stat_halfeye(mapping=aes(y=logit), data=obs_rng) +
  facet_grid(Test~.)+
  scale_y_continuous(breaks = qlogis(c(0.01, .1, .5, .9, .99, .999)), 
                     labels = c(0.01, .1,.5, .9, .99, .999),
                     limits = qlogis(c(0.0005, .999995))) +
  xlab('') + ylab('') +
  coord_flip() +
  theme_minimal() +
  theme(#text = element_text('serif', size = 9)
        plot.tag = element_text('serif', size = 9),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()
        )
obs_logit_plt
  
```

To correct for the correlation of _obs_ and test values, we additionally regressed _obs_ on some characteristics that may suggest TBM and thus induce a TBM mycobacterial tests. In this analysis, we considered HIV, TB-suggestive symptoms and duration, Pulmonary and miliary TB on X-ray image.

$$
  \mathrm{logit}(P_{obs}) \sim X_{obs}^T \beta_{obs}
$$
     where $X_{obs}$ is the relevant characteristics that raise the suspicion of TBM, $\beta_{obs}$ is the vector of coefficients.

### Imputation model

To diagnose the validity and impact of our imputation model, we used three different checks. First, we visually compared the distributions of the imputed data and observed data, conditional on the response propensity. To do this, for each variable of interest $Y_v$ we fitted a logistic regression with the indicator of missingness as the response variable and the completed variables apart from $Y_v$ (denoted as $Y_{-v}$) as the independent variables. We plotted $Y_v$ against the propensity of response ($e_v$), using different colours for imputed and observed values of $Y_v$. Second, we compared the residual of $Y_v^{imputed}$ and $Y_v^{observed}$ conditionally on $e_v$. To do this, we fitted a generalised linear model with Gaussian family with identity link function for continuous variables and Binomial with logit link function for binary variables. The results were demonstrated using a kernel density plot for the former and smoothing loess curves for the other. Third, we performed a posterior predictive check in which we compared the posterior distribution of the coefficients of interest in the prevalence model based on the completed datasets (which contained observed and imputed values) and 100 replicated datasets in which all variables were imputed based on the imputation model. For a good imputation we would expect the likelihood to witness the former is not extreme. The details of the methods are outlined and discussed elsewhere [@nguyen2017; @bondarenko2016]. For simplicity we only analysed variables that had more than 10 missing observations.

## Results

### Latent class model

The expected residual correlations between 3 pairs of confirmatory tests for 5 model choices are shown in Supplementary Figure \@ref(fig:resid-pwcorr). Except for model 1 which underestimated the observed values, other models that incorporated random effects showed a good correspondence between predicted and observed correlation, in which case all residual values fluctuated around 0. This suggested that a correction for local dependence was indeed necessary.

```{r resid-pwcorr, fig.align='center', fig.cap="Residual correlation plots for five different model choices", fig.id="resid-pwcorr", warning=FALSE, out.width='50%', fig.width=4, message=FALSE}
rpc_plot = readRDS(file.path(data_dir, '..', 'export', 'metrics', 'rpc_plot.RDS'))
# rpc_plot + scale_color_brewer(type='qual', palette = 2) +  theme(legend.position = 'bottom')
ggplot(data=rpc_plot$data, aes(x=Pair, color=Model, group=Model)) + 
  geom_line(aes(y=mean), stat='summary', linewidth = .7)+
  # stat_summary(aes(y=mean), geom='line', fun=sum) +
  scale_y_continuous(limits=c(-.12, .12)) +
  scale_x_discrete(expand=c(.07,.07)) +
  scale_color_brewer(palette='Set1') +
  theme_bw() + 
  theme(axis.title = element_blank(), legend.position = 'bottom')
```

```{r plot-missing, out.width='100%', fig.dim=c(10, 12.5), fig.align='center', fig.cap='Estimates of model 3 (denoted as Selected model), model 3 with missing Xpert (denoted as Incomplete Xpert), and model 3 with missing Xpert and a wider prior for test specificity (denoted as Incomplete Xpert + Wide prior). From top to bottom are estimates of prevalence model, bacillary burden model, and sensitivity (TPR) and (1 - Specificity) (FPR) for 3 confirmatory tests and the response rates of Xpert for TBM and non-TBM patients. In all plots: dots, thick lines, and thin lines are the means, 50\\%, and 95\\% credible intervals.'}

plot_m <- readRDS(file.path(data_dir, '..', 'export', 'm3m_plot.RDS'))
z_plot <- readRDS(file.path(data_dir, '..', 'export', 'z_plot_u.RDS'))

wrap_plots(plot_m[names(plot_m)!='z_plot'], ncol=1) + z_plot + 
  plot_layout(guide='collect', heights = c(7,3,3)) +
  plot_annotation(tag_levels = 'A') & theme(legend.position='bottom')

```

There are no large differences in the posterior estimates of the model with missing Xpert and the selected model (model 3) (Supplementary Figure \@ref(fig:plot-missing)), except for cranial nerve palsy and past noticed TB contact. Overall, the estimates of the selected model is shrunk more towards 0. The response rate for Xpert in the non-TBM group is estimated as `r plot_m$z_plot$data |> filter(parameter == 'z_observe[1]' & Model == "Incomplete Xpert") |> select(m) |> unlist() |> plogis() |> round(2)` (`r plot_m$z_plot$data |> filter(parameter == 'z_observe[1]' & Model == "Incomplete Xpert") |> select(ll) |> unlist() |> plogis() |> round(2)` - `r plot_m$z_plot$data |> filter(parameter == 'z_observe[1]' & Model == "Incomplete Xpert") |> select(hh) |> unlist() |> plogis() |> round(2)`). Almost all TBM patients were tested.

### Imputation of covariables

```{r load_impute_data}
library(ggfx)
load(file.path(data_dir, '..', 'export', 'impute_chk_plot_hex.Rdata'))
```

We can see there are no systematic difference in imputed and observed values conditioned on response propensity for HIV status, symptom duration, and RGCS. There are some discrepancies in the two other variables, whereas the imputed values are consistently lower than observed values. This is expected, as a composite variable is considered unobserved if and only if all of its compartments are either missing or negative (i.e. a MNAR situation). Similar trends are also showed in the residuals against response propensity scores (Supplementary Figure \@ref(fig:impute-plot)). In Supplementary Figure \@ref(fig:impute-ppc), for all coefficients and the intercept, both the posterior distribution and estimated median of the completed dataset do not fall out of the plausible counterparts based on the fully imputed datasets. This suggests that the difference of imputed and observed values might not heavily impact the posterior estimation. The most remarkable shift is TB-suggestive symptoms, of which the estimate of the completed dataset is marginally lower than those of the replicated ones. However, its median is still within the 95% credible intervals and its posterior distribution still overlaps heavily the replicated datasets.

```{r impute-plot, out.width='75%', fig.dim=c(8,12), fig.align='center', fig.cap = "Diagnostic plots for the imputation model. On the left are hexagonal heatmaps between the reponse propensity and the values observed or imputed, centred and standardised. On the right hand side are the kernel density plots of the residuals for continuous variables and smoothed curve against the response propensity for binary variables of the regression model $y \\sim e_y$ where $y$ is the variable of interest and $e_y$ is the response propensity of that variable."}

hiv_plot$name = 'HIV +'
cs_plot$name = 'TB-suggestive symptoms'
mp_plot$name = 'Focal neurological deficit'
gcs_plot$name = 'RGCS (centred)'
id_plot$name = 'log(Symptom duration, days)'
impute_plots <- 
  c(
    lapply(
      list(hiv_plot, cs_plot, mp_plot),
      function(plt){
       # wrap_elements(grid::textGrob(plt$name, rot=90)) + 
          wrap_ggplot_grob(plt$plot1)  +
          wrap_ggplot_grob(plt$plot3) + plot_layout(widths=c(1,1))
        # wrap_elements(gridExtra::arrangeGrob(, nrow=1, left=plt$name))
      }
    ),
    lapply(
      list(id_plot, gcs_plot),
      function(plt){
       # wrap_elements(grid::textGrob(plt$name, rot=90)) + 
          wrap_ggplot_grob(plt$plot1)  +
          wrap_ggplot_grob(plt$plot2) + plot_layout(widths=c(1,1))
        # wrap_elements(gridExtra::arrangeGrob(, nrow=1, left=plt$name))
      }
    )
  )

plt <- wrap_plots(impute_plots, ncol=1) + plot_annotation(tag_levels = list("HIV", "TB_symptoms", "Local_neuro-defict", "log(Symptom duration, days)", "RGCS (centred)")) + plot_layout(guides='collect')
  
plt
```

```{r impute-ppc, out.width='100%', fig.dim=c(12,9), fig.cap = "Posterior distributions of parameters for variables with missing values in the prevalence model. The curves are the posterior distributions of the completed dataset in red and 100 fully imputed datasets in grey. The red point is the posterior median of the completed dataset. The grey line is the 95\\% confidence interval of posterior medians of the fully imputed dataset."}
ppc <- readRDS(file.path(data_dir,'../export/ppc_checkresults.RDS'))
vars <- c('HIV' = 'a1', 
          'TB-suggestive symptoms' = 'a2',
          'Focal neurological deficit' = 'a3',
          'log(Symptom duration, days)' = 'a11', 
          'RGCS ' = 'a18',
          'Intercept' = 'a0')
for (a in seq_along(vars)) ppc[[vars[[a]]]] <- patchwork::wrap_elements(ppc[[vars[[a]]]]) + ggtitle(names(vars)[[a]])
patchwork::wrap_plots(ppc, ncol=2, nrow=3) 


```
